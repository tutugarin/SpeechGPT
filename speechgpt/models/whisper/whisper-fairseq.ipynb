{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo fairseq transformers huggingface-hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom fairseq.models import FairseqEncoder, FairseqDecoder, FairseqEncoderDecoderModel, register_model, FairseqModel\nfrom transformers.models.whisper.modeling_whisper import WhisperPositionalEmbedding, WhisperSdpaAttention\nfrom transformers.activations import GELUActivation\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\nfrom fairseq.data import Dictionary\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import AutoProcessor\nfrom torchaudio.transforms import MelSpectrogram\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom typing import Optional, Tuple, Union\nimport logging\nimport os\nimport sys\nfrom torch import nn, Tensor\nfrom fairseq import utils\nfrom typing import Dict\nimport math\nimport torch\nfrom torchaudio.transforms import MelSpectrogram\nimport torch\nfrom IPython.display import Audio\nimport soundfile as sf\n\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\n\ntry:\n    from transformers import WhisperForConditionalGeneration, WhisperTokenizer, WhisperConfig\n    has_hf = True\nexcept ImportError:\n    has_hf = False\n\nlogger = logging.getLogger(__name__)\n\nfrom fairseq.models import FairseqEncoder, FairseqDecoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:28:02.749851Z","iopub.execute_input":"2024-11-17T11:28:02.750277Z","iopub.status.idle":"2024-11-17T11:28:50.930351Z","shell.execute_reply.started":"2024-11-17T11:28:02.750234Z","shell.execute_reply":"2024-11-17T11:28:50.929248Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n#костыль  \nclass DummyEncoder(FairseqEncoder):\n    def forward(self, *args, **kwargs):\n        return None  # или возвращайте данные, если требуется.\n\n#костыль  \nclass DummyDecoder(FairseqDecoder):\n    def forward(self, *args, **kwargs):\n        return None\n\n@register_model(\"whisper-turbo\")\nclass HuggingFaceWhisperModel(FairseqEncoderDecoderModel):\n    def __init__(self, args):\n        dummy_encoder = DummyEncoder(None)\n        dummy_decoder = DummyDecoder(None)\n        super().__init__(dummy_encoder, dummy_decoder)  # No encoder or decoder\n        if not has_hf:\n            raise ImportError(\n                '\\n\\nPlease install huggingface/transformers with:'\n                '\\n\\n  pip install transformers'\n                '\\n\\nOr to make local edits, install the submodule:'\n                '\\n\\n  git submodule update --init '\n                'fairseq/models/huggingface/transformers'\n            )\n        self.args = args\n        self.load_model(args)\n\n    def load_model(self, args):\n        model_path = getattr(args, 'load_hf_whisper_from', '')\n        assert model_path, \"Model path must be specified in --load-hf-whisper-from\"\n        self.model = WhisperForConditionalGeneration.from_pretrained(model_path)\n        self.processor = AutoProcessor.from_pretrained(model_path)\n        self.config = self.model.config\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument('--load-hf-whisper-from', type=str, default='',\n                            help='load Hugging Face pretrained Whisper from path')\n        parser.add_argument('--max-target-positions', type=int,\n                            help='maximum target positions for the decoder')\n\n    @classmethod\n    def build_model(cls, args, task):\n        default_architecture(args)\n        return cls(args)\n\n    def forward(\n        self,\n        src_tokens: Tensor,\n        tgt_tokens: Optional[Tensor] = None,\n        src_lengths: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n    ):\n        input_features = src_tokens \n        \n        if tgt_tokens is not None:\n            outputs = self.model(\n                input_features=input_features,\n                decoder_input_ids=tgt_tokens,\n            )\n            logits = outputs.logits\n        else:\n            self.eval()\n            with torch.no_grad():\n                generated_ids = self.model.generate(\n                    input_features=input_features\n                )\n            logits = generated_ids\n    \n        batch_size = input_features.size(0)\n        seq_len = input_features.size(1)\n        \n        # Dummy values for hidden states and cells (as Whisper doesn't have these)\n        # final_hiddens = torch.zeros(1, batch_size, self.config.d_model).to(input_features.device)\n        # final_cells = torch.zeros(1, batch_size, self.config.d_model).to(input_features.device)\n        # encoder_padding_mask = torch.zeros(seq_len, batch_size).to(input_features.device)\n    \n        return tuple(\n            (\n                logits,  # seq_len x batch x hidden (logits or other output)\n                # final_hiddens,  # Dummy hidden states\n                # final_cells,  # Dummy cell states\n                # encoder_padding_mask,  # Dummy padding mask\n                None,\n                None,\n                None\n            )\n        )\n\n    def parse_waveform(self, file: str, **kwargs):\n        waveform, sampling_rate = sf.read(file)\n        waveform = torch.tensor(waveform).unsqueeze(0).float() \n        inputs = self.processor(waveform.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\")\n        waveform = inputs['input_features']\n        \n        return waveform\n\n\n\n    def generate(self, audio_tokens=None, text=False, skip_special_tokens=True, file=None, **kwargs):\n        if audio_tokens is None and file is None:\n            raise Exception(\"audio_tokens or file must not be None\")\n        \n        if file is not None:\n            waveform = self.parse_waveform(file)\n            audio_tokens = waveform\n            \n        self.eval()\n        \n        with torch.no_grad():\n            generated_ids = self.model.generate(audio_tokens, **kwargs)\n\n        if not text:\n            return generated_ids\n\n        return self.processor.batch_decode(generated_ids, skip_special_tokens)\n\ndef default_architecture(args):\n    args.load_hf_whisper_from = getattr(args, 'load_hf_whisper_from', 'openai/whisper-large-v3-turbo')\n    args.generate_text = getattr(args, 'generate_text', 'False')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:28:50.932373Z","iopub.execute_input":"2024-11-17T11:28:50.933602Z","iopub.status.idle":"2024-11-17T11:28:50.959475Z","shell.execute_reply.started":"2024-11-17T11:28:50.933540Z","shell.execute_reply":"2024-11-17T11:28:50.958300Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Args:\n    pass\n\nmodel = HuggingFaceWhisperModel.build_model(Args(), None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:30:25.362423Z","iopub.execute_input":"2024-11-17T11:30:25.363221Z","iopub.status.idle":"2024-11-17T11:30:29.008892Z","shell.execute_reply.started":"2024-11-17T11:30:25.363176Z","shell.execute_reply":"2024-11-17T11:30:29.007870Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Загружаем тестовый датасет\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n\nwaveform = torch.tensor(sample['array']).unsqueeze(0)  # Add batch dimension\nsampling_rate = sample['sampling_rate']\n\nwaveform = waveform.float()\n\ninputs = model.processor(waveform.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\")\nwaveform = inputs['input_features']\n\nsf.write('audio.wav',sample['array'], sampling_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:30:30.468936Z","iopub.execute_input":"2024-11-17T11:30:30.469409Z","iopub.status.idle":"2024-11-17T11:30:31.343075Z","shell.execute_reply.started":"2024-11-17T11:30:30.469366Z","shell.execute_reply":"2024-11-17T11:30:31.342028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# сгенерировать токены\n\nmodel.generate(waveform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:30:31.344947Z","iopub.execute_input":"2024-11-17T11:30:31.345382Z","iopub.status.idle":"2024-11-17T11:31:19.386837Z","shell.execute_reply.started":"2024-11-17T11:30:31.345342Z","shell.execute_reply":"2024-11-17T11:31:19.385642Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n           264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n          5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n          2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n            13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n           264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n           949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n          3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n         12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n           534, 10281,   934,   439,    11]])"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# сгенерировать текст\n\nmodel.generate(waveform, text=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:31:19.389392Z","iopub.execute_input":"2024-11-17T11:31:19.389987Z","iopub.status.idle":"2024-11-17T11:32:05.241003Z","shell.execute_reply.started":"2024-11-17T11:31:19.389912Z","shell.execute_reply":"2024-11-17T11:32:05.239850Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\"]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# форвард пасс\n\nin_features = model.processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\nprompt_ids = torch.tensor(model.processor.tokenizer.prefix_tokens).unsqueeze(0)\nmodel(src_tokens=in_features, tgt_tokens=prompt_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:32:05.242324Z","iopub.execute_input":"2024-11-17T11:32:05.242704Z","iopub.status.idle":"2024-11-17T11:32:28.246524Z","shell.execute_reply.started":"2024-11-17T11:32:05.242658Z","shell.execute_reply":"2024-11-17T11:32:28.245413Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 2.4991,  1.7261, -0.9168,  ...,  0.1926,  2.3552, -0.5627],\n          [-0.7742, -0.2981, -2.3134,  ..., -2.1932, -2.1076, -3.5427]]],\n        grad_fn=<UnsafeViewBackward0>),\n None,\n None,\n None)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# сгенерировать из текст аудиофайла\n\nmodel.generate(file='audio.wav', text=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:32:28.249188Z","iopub.execute_input":"2024-11-17T11:32:28.249830Z","iopub.status.idle":"2024-11-17T11:33:12.871566Z","shell.execute_reply.started":"2024-11-17T11:32:28.249775Z","shell.execute_reply":"2024-11-17T11:33:12.870448Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\"]"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# сгенерировать токены из аудиофайла\n\nmodel.generate(file='audio.wav')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:33:12.873148Z","iopub.execute_input":"2024-11-17T11:33:12.873530Z","iopub.status.idle":"2024-11-17T11:34:00.688697Z","shell.execute_reply.started":"2024-11-17T11:33:12.873492Z","shell.execute_reply":"2024-11-17T11:34:00.687517Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n           264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n          5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n          2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n            13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n           264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n           949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n          3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n         12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n           534, 10281,   934,   439,    11]])"},"metadata":{}}],"execution_count":14}]}