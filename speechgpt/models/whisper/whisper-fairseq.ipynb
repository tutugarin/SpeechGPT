{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Whisper Fairseq Adaptation","metadata":{}},{"cell_type":"code","source":"!pip install torchinfo fairseq transformers huggingface-hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom fairseq.models import FairseqEncoder, FairseqDecoder, FairseqEncoderDecoderModel, register_model\nfrom transformers.models.whisper.modeling_whisper import WhisperPositionalEmbedding, WhisperSdpaAttention\nfrom transformers.activations import GELUActivation\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\nfrom fairseq.data import Dictionary\nimport torchaudio\nfrom datasets import load_dataset\nfrom transformers import AutoProcessor\nfrom torchaudio.transforms import MelSpectrogram\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:18:12.800602Z","iopub.execute_input":"2024-11-03T07:18:12.801452Z","iopub.status.idle":"2024-11-03T07:18:49.624508Z","shell.execute_reply.started":"2024-11-03T07:18:12.801402Z","shell.execute_reply":"2024-11-03T07:18:49.623444Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 1. Загружаем Whisper с HF\n\nprocessor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\")\nw = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3-turbo\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muuNKe5WdwKU","outputId":"8ea00da5-0a7a-4382-841d-523df74c5a31","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:18:49.625743Z","iopub.execute_input":"2024-11-03T07:18:49.626463Z","iopub.status.idle":"2024-11-03T07:19:00.110045Z","shell.execute_reply.started":"2024-11-03T07:18:49.626424Z","shell.execute_reply":"2024-11-03T07:19:00.109102Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 2. Сохраняем веса модели\n\ntorch.save(w.state_dict(), \"whisper_large_v3_turbo_weights.pth\")","metadata":{"id":"Cwe8fxasgH-c","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:19:00.113061Z","iopub.execute_input":"2024-11-03T07:19:00.113683Z","iopub.status.idle":"2024-11-03T07:19:05.119869Z","shell.execute_reply.started":"2024-11-03T07:19:00.113636Z","shell.execute_reply":"2024-11-03T07:19:05.118768Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 3. Архитектура модели для Fairseq\n\nclass WhisperEncoder(FairseqEncoder):\n    def __init__(self, dictionary, embed_dim=1280, num_layers=32):\n        super().__init__(dictionary)\n        self.conv1 = nn.Conv1d(128, embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n        self.embed_positions = nn.Embedding(1500, embed_dim)\n        self.layers = nn.ModuleList([\n            WhisperEncoderLayer(embed_dim) for _ in range(num_layers)\n        ])\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        #print(\"self.conv1(x)\", x.shape) \n        \n        x = self.conv2(x)\n        #print(\"self.conv2(x)\", x.shape) \n        \n        seq_length = x.size(2)\n    \n        positions = torch.arange(seq_length, device=x.device).unsqueeze(0) \n        #print(\"positions\", positions.shape)  # Should be [1, seq_length2]\n    \n        ep = self.embed_positions(positions)  # Shape: [1, seq_length2, embed_dim]\n        \n        x = x.permute(0, 2, 1) \n        x = x + ep  \n    \n        for layer in self.layers:\n            x = layer(x)\n    \n        x = self.layer_norm(x)\n        return x\n\n\n# Define WhisperEncoderLayer\n\nclass WhisperEncoderLayer(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.self_attn = WhisperAttention(embed_dim)\n        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n        self.activation_fn = GELUActivation()\n        self.fc1 = nn.Linear(embed_dim, 5120)\n        self.fc2 = nn.Linear(5120, embed_dim)\n        self.final_layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        residual = x\n        x = self.self_attn(x)\n        x = self.self_attn_layer_norm(x + residual)\n\n        residual = x\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.final_layer_norm(x + residual)\n        return x\n\n\nclass WhisperAttention(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(x.size(-1))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, v)\n\n        return self.out_proj(attn_output)\n\n\nclass WhisperDecoder(FairseqDecoder):\n    def __init__(self, dictionary, embed_dim=1280, num_layers=4):\n        super().__init__(dictionary)\n        self.embed_tokens = nn.Embedding(len(dictionary), embed_dim, padding_idx=50257)\n        self.embed_positions = WhisperPositionalEmbedding(448, embed_dim)\n\n        self.layers = nn.ModuleList([\n            WhisperDecoderLayer(embed_dim) for _ in range(num_layers)\n        ])\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, encoder_out):\n        x = x.long()\n        #print(\"x\", x.shape)\n        \n        batch_size = x.size()[0]\n        seq_length = x.size()[1] \n        positions = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, -1)\n    \n        #print(\"positions\", positions.shape)\n        et = self.embed_tokens(x)\n        ep = self.embed_positions(positions)\n    \n        ep = ep.unsqueeze(1).expand(-1, 3000, -1)  # Adjusted this line\n        #print(\"et\", et.shape)\n        #print(\"ep\", ep.shape)\n    \n        # Embed tokens and positions\n        x = et + ep\n    \n        #print(\"self.embed_tokens(x) + self.embed_positions(positions)\", x.shape)\n        \n        # Pass through the decoder layers\n        for layer in self.layers:\n            x = layer(x, encoder_out)\n    \n        # Apply layer normalization\n        x = self.layer_norm(x)\n        \n        return x\n\nclass WhisperDecoderLayer(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.self_attn = WhisperAttention(embed_dim)\n        self.activation_fn = GELUActivation()\n        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n        self.encoder_attn = WhisperAttention(embed_dim)\n        self.encoder_attn_layer_norm = nn.LayerNorm(embed_dim)\n        self.fc1 = nn.Linear(embed_dim, 5120)\n        self.fc2 = nn.Linear(5120, embed_dim)\n        self.final_layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, encoder_out):\n        residual = x\n        x = self.self_attn(x)\n        x = self.self_attn_layer_norm(x + residual)\n\n        residual = x\n        x = self.encoder_attn(x, encoder_out)\n        x = self.encoder_attn_layer_norm(x + residual)\n\n        residual = x\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        x = self.final_layer_norm(x + residual)\n        return x\n\n\nclass WhisperForConditionalGeneration(nn.Module):\n    def __init__(self, dictionary, embed_dim=1280, encoder_layers=32, decoder_layers=4):\n        super().__init__()\n        self.model = WhisperModel(dictionary, embed_dim=embed_dim, encoder_layers=encoder_layers, decoder_layers=decoder_layers)\n        self.proj_out = nn.Linear(in_features=1280, out_features=len(dictionary), bias=False)\n\n    def forward(self, src_tokens, prev_output_tokens):\n        encoder_out = self.model.encoder(src_tokens)\n        decoder_out = self.model.decoder(prev_output_tokens, encoder_out=encoder_out)\n        logits = self.proj_out(decoder_out)\n        return logits\n\n\n@register_model('whisper-large-v3-turbo')\nclass WhisperModel(FairseqEncoderDecoderModel):\n    def __init__(self, dictionary, embed_dim=1280, encoder_layers=32, decoder_layers=4):\n        encoder = WhisperEncoder(dictionary, embed_dim=embed_dim, num_layers=encoder_layers)\n        decoder = WhisperDecoder(dictionary, embed_dim=embed_dim, num_layers=decoder_layers)\n        super().__init__(encoder, decoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        dictionary = task.source_dictionary\n        return cls(dictionary=dictionary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:19:05.122909Z","iopub.execute_input":"2024-11-03T07:19:05.123709Z","iopub.status.idle":"2024-11-03T07:19:05.155770Z","shell.execute_reply.started":"2024-11-03T07:19:05.123654Z","shell.execute_reply":"2024-11-03T07:19:05.154657Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 4. Перенесем словарь токенов\n\nfairseq_dictionary = Dictionary()\nfairseq_dictionary.symbols = []\ntokenizer = processor.tokenizer\nvocab = tokenizer.get_vocab()\n\nfor token in vocab.keys():\n    fairseq_dictionary.add_symbol(token)\n\nprint(f\"Number of tokens in Fairseq dictionary: {len(fairseq_dictionary)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqosVZw6D8F3","outputId":"a0601214-48a4-45fe-eac8-5792c550db1a","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:19:05.157318Z","iopub.execute_input":"2024-11-03T07:19:05.157805Z","iopub.status.idle":"2024-11-03T07:19:05.271308Z","shell.execute_reply.started":"2024-11-03T07:19:05.157758Z","shell.execute_reply":"2024-11-03T07:19:05.270219Z"}},"outputs":[{"name":"stdout","text":"Number of tokens in Fairseq dictionary: 51866\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 5. Создаем модель\n\nmodel = WhisperForConditionalGeneration(dictionary=fairseq_dictionary)\n\nmodel","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFUU667uRCdF","outputId":"c5f40250-2d4d-446e-ebf0-5076335bc819","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:19:05.273381Z","iopub.execute_input":"2024-11-03T07:19:05.273831Z","iopub.status.idle":"2024-11-03T07:19:13.181332Z","shell.execute_reply.started":"2024-11-03T07:19:05.273783Z","shell.execute_reply":"2024-11-03T07:19:13.180307Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 1280)\n      (layers): ModuleList(\n        (0-31): 32 x WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n      (layers): ModuleList(\n        (0-3): 4 x WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# 6. Загружаем сохраненные веса\nstate_dict = torch.load(\"whisper_large_v3_turbo_weights.pth\", weights_only=True)\n\nmodel.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:19:13.182877Z","iopub.execute_input":"2024-11-03T07:19:13.183670Z","iopub.status.idle":"2024-11-03T07:19:15.407040Z","shell.execute_reply.started":"2024-11-03T07:19:13.183617Z","shell.execute_reply":"2024-11-03T07:19:15.405963Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 7. Переносим на GPU, если оно есть\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Using device: {device}\")\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:19:38.879729Z","iopub.execute_input":"2024-11-03T07:19:38.880143Z","iopub.status.idle":"2024-11-03T07:19:38.900897Z","shell.execute_reply.started":"2024-11-03T07:19:38.880107Z","shell.execute_reply":"2024-11-03T07:19:38.899889Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 1280)\n      (layers): ModuleList(\n        (0-31): 32 x WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n      (layers): ModuleList(\n        (0-3): 4 x WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n)"},"metadata":{}}],"execution_count":10}]}