{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti0sq3xLw0vW",
        "outputId": "70e42761-6cd0-438d-baf0-f1b710a21108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip==24.0 in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pip==24.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo fairseq transformers huggingface-hub datasets==3.0.1"
      ],
      "metadata": {
        "id": "qLA3M-r7xhzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.models import BaseFairseqModel, register_model\n",
        "from torch import Tensor\n",
        "from typing import Optional, Dict\n",
        "\n",
        "from speechgpt.models.whisper.model import HuggingFaceWhisperModel\n",
        "# импортировать свою модель\n",
        "\n",
        "\n",
        "# класс для аргументов\n",
        "class Args:\n",
        "    pass\n",
        "\n",
        "@register_model(\"asr-llm-cascade-model\")\n",
        "class AsrLlmCascadeModel(BaseFairseqModel):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.asr = None\n",
        "        self.load_models(args)\n",
        "\n",
        "    def load_models(self, args):\n",
        "        self.asr = HuggingFaceWhisperModel.build_model(Args, None)\n",
        "        # self.llm = добавить модель\n",
        "\n",
        "    @classmethod\n",
        "    def build_model(cls, args=None, task=None):\n",
        "        args = args or Args()\n",
        "        return cls(args)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens: Tensor,\n",
        "        tgt_tokens: Optional[Tensor] = None,\n",
        "        src_lengths: Optional[Tensor] = None,\n",
        "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
        "    ):\n",
        "        \"\"\"Форвард пасс метод (может использоваться при обучения, но для генерации\n",
        "        использовать generate\"\"\"\n",
        "        whisper_output = self.asr(src_tokens, tgt_tokens, src_lengths, incremental_state)\n",
        "        # добавть работу с моделью llm_output = self.llm(whisper_output, ...)\n",
        "\n",
        "        # возвращать llm output\n",
        "        return whisper_output\n",
        "\n",
        "\n",
        "    def generate(self, input_tokens=None, text=False, skip_special_tokens=True, file=None, **kwargs):\n",
        "\n",
        "        if input_tokens is None and file is None:\n",
        "            raise Exception(\"input_tokens or file must not be None\")\n",
        "\n",
        "        whisper_output = self.asr.generate(input_tokens, text, skip_special_tokens, file, **kwargs)\n",
        "        # добавть работу с моделью llm_output = self.llm(whisper_output, ...)\n",
        "\n",
        "        # возвращать llm output\n",
        "        return whisper_output"
      ],
      "metadata": {
        "id": "4FCzbSTYw4ns"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cascade = AsrLlmCascadeModel.build_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2VoMYexw4qd",
        "outputId": "ddf8989e-61dd-4b84-ade2-0d700b2e38d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тесты, что работает"
      ],
      "metadata": {
        "id": "_cyESGHlEgnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем тестовый датасет\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import soundfile as sf\n",
        "\n",
        "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
        "sample = dataset[0][\"audio\"]\n",
        "\n",
        "\n",
        "waveform = torch.tensor(sample['array']).unsqueeze(0)  # Add batch dimension\n",
        "sampling_rate = sample['sampling_rate']\n",
        "\n",
        "waveform = waveform.float()\n",
        "\n",
        "inputs = cascade.asr.processor(waveform.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "waveform = inputs['input_features']\n",
        "\n",
        "sf.write('audio.wav',sample['array'], sampling_rate)"
      ],
      "metadata": {
        "id": "WnvxIl1Gw4s6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. сгенерировать токены\n",
        "\n",
        "cascade.generate(waveform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOrhG4v5w4wV",
        "outputId": "31f0312c-ca0a-4ab7-c54a-69b1d5b82491"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n",
              "           264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n",
              "          5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n",
              "          2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n",
              "            13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n",
              "           264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n",
              "           949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n",
              "          3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n",
              "         12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n",
              "           534, 10281,   934,   439,    11]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. сгенерировать текст\n",
        "\n",
        "cascade.generate(waveform, text=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXHz1DxnDr-a",
        "outputId": "3cfaebbd-6d99-40b8-9155-e72a3119859e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\"]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. форвард пасс\n",
        "\n",
        "in_features = cascade.asr.processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
        "prompt_ids = torch.tensor(cascade.asr.processor.tokenizer.prefix_tokens).unsqueeze(0)\n",
        "cascade(src_tokens=in_features, tgt_tokens=prompt_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2jGeV7pD1qE",
        "outputId": "5ce6cc6b-ba24-4996-8040-a44d36ecc243"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2.4991,  1.7260, -0.9168,  ...,  0.1925,  2.3552, -0.5627],\n",
              "          [-0.7743, -0.2981, -2.3134,  ..., -2.1932, -2.1076, -3.5427]]],\n",
              "        grad_fn=<UnsafeViewBackward0>),\n",
              " None,\n",
              " None,\n",
              " None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. сгенерировать из текст аудиофайла\n",
        "\n",
        "cascade.generate(file='audio.wav', text=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szXmyx-zD4vS",
        "outputId": "38bdc83c-4605-4cfb-a18f-2623129db679"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. сгенерировать токены из аудиофайла\n",
        "\n",
        "cascade.generate(file='audio.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz63pUNKD6k8",
        "outputId": "caab4185-6d0e-432d-8d11-9e2fae2b9056"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n",
              "           264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n",
              "          5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n",
              "          2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n",
              "            13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n",
              "           264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n",
              "           949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n",
              "          3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n",
              "         12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n",
              "           534, 10281,   934,   439,    11]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}