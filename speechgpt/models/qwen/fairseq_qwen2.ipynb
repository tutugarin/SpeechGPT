{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install pip==24.0\n",
    "# !pip install -q torch==2.0.0 --extra-index-url https://download.pytorch.org/whl/cu118 --user\n",
    "# !pip install -q transformers fairseq --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "agvNHVfc4N5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 16:16:10 | INFO | speechgpt_logger | Loading model from Qwen/Qwen2-0.5B\n",
      "2024-12-30 16:16:11 | INFO | speechgpt_logger | Initializing Qwen2Decoder with 24 layers.\n",
      "2024-12-30 16:16:14 | INFO | speechgpt_logger | Qwen2Decoder initialized successfully.\n",
      "2024-12-30 16:16:15 | INFO | speechgpt_logger | Model initialized successfully.\n",
      "2024-12-30 16:16:15 | INFO | speechgpt_logger | Loading model weights.\n",
      "2024-12-30 16:16:19 | INFO | speechgpt_logger | Loaded model weights.\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from model import HuggingFaceQwen2ForCausalLM\n",
    "\n",
    "args = OmegaConf.create()\n",
    "args.llm_config = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "model = HuggingFaceQwen2ForCausalLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is attention mask in language model?in language model?\n",
      "\n",
      "Attention mask is a computational device or filter that is used to reduce the dimensionality of a set of tokens in the context in which they appear in a sentence. It is often used in natural language processing (NLP) to improve the accuracy of the model in predicting future words or sentences. It is also known as an attention mask or attention mechanism. The idea is to capture the importance of the context in the passage and make the words more attentional.\n",
      "_________________________________________________\n",
      "Why language models are successful?\n",
      "The concept of \"Language Models\" is quite controversial and complicated, but one possible explanation is to consider that the \"true\" language model, the word model, is a particular special kind of hidden Markov Model. Then the question why it is so successful is the following: why are there so many languages?\n",
      "\n",
      "You should define what's special about language models and compare them to any other kind of hidden Markov Models. \n",
      "For example, Hidden Markov Models are the hidden layer of a Markov Chain and if you look up the Wikipedia article for Hidden Markov Model, it says the following\n",
      "...it is, however, not very informative since the underlying data is used only to build and condition the Markov chain. \n",
      "\n",
      "See also these\n",
      "_________________________________________________\n",
      "Which paper is fundamental in nowadays NLP? In the NLP field, it is common to hear a few words describing paper 1 and paper 2. This often implies that the two papers are from the same journal. \n",
      "\n",
      "Do we need to consider what has been published since 1948? What paper are we actually talking about in this context? \n",
      "\n",
      "My understanding is that one can have a journal that has articles published in other journals - however, I am not sure about the other papers that have been published in the 1940s. \n",
      "\n",
      "I am not aware if one can say that these papers are fundamental, as I can not find these names in the titles or in the abstracts of the papers. Is it possible to have some names in the titles,\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"What is attention mask in language model?\",\n",
    "    \"Why language models are successful?\",\n",
    "    \"Which paper is fundamental in nowadays NLP?\",\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.llm_config)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tok_outs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "input_ids = tok_outs.input_ids\n",
    "attention_mask = tok_outs.attention_mask\n",
    "\n",
    "generate_ids = model.generate(input_ids, attention_mask=attention_mask,\n",
    "                              max_new_tokens=150, do_sample=True, top_k=50, top_p=0.95)\n",
    "gen_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "for _ in gen_texts:\n",
    "    print(_)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.4256,  4.8533,  3.4071,  ..., -4.8739, -4.8724, -4.8735],\n",
       "         [ 1.0334,  2.2287,  1.7907,  ..., -5.4180, -5.4191, -5.4189],\n",
       "         [ 7.9500,  5.3180,  4.9196,  ..., -4.9389, -4.9381, -4.9378],\n",
       "         ...,\n",
       "         [ 6.6885,  4.1731,  2.9295,  ..., -5.6208, -5.6225, -5.6211],\n",
       "         [ 4.6720,  2.8864,  2.7626,  ..., -5.7757, -5.7758, -5.7749],\n",
       "         [ 6.9488,  5.8876,  6.5815,  ..., -3.2889, -3.2888, -3.2876]],\n",
       "\n",
       "        [[ 7.8377,  3.2649,  2.3366,  ..., -5.3130, -5.3133, -5.3130],\n",
       "         [ 5.4114,  5.7092,  3.6707,  ..., -5.2595, -5.2586, -5.2577],\n",
       "         [ 7.5345,  7.0438,  3.4638,  ..., -4.9855, -4.9854, -4.9849],\n",
       "         ...,\n",
       "         [ 7.0046,  5.6227,  7.7066,  ..., -3.6718, -3.6726, -3.6713],\n",
       "         [ 7.2191,  5.7511,  7.5483,  ..., -3.6273, -3.6283, -3.6269],\n",
       "         [ 7.0624,  5.7042,  6.8024,  ..., -3.6658, -3.6667, -3.6655]],\n",
       "\n",
       "        [[ 4.6012,  2.8672,  1.2418,  ..., -4.7340, -4.7324, -4.7334],\n",
       "         [ 5.2055,  5.2384,  5.1805,  ..., -3.8021, -3.8019, -3.8027],\n",
       "         [ 2.2612,  4.2517,  2.6010,  ..., -5.0426, -5.0431, -5.0422],\n",
       "         ...,\n",
       "         [ 7.8117,  4.4671,  5.6315,  ..., -4.7784, -4.7773, -4.7774],\n",
       "         [ 8.1697,  6.0092,  6.1033,  ..., -5.2642, -5.2646, -5.2643],\n",
       "         [ 5.0500,  1.9037,  2.1297,  ..., -6.1501, -6.1503, -6.1495]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
