{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install pip==24.0\n",
    "# !pip install -q torch==2.0.0 --extra-index-url https://download.pytorch.org/whl/cu118 --user\n",
    "# !pip install -q transformers fairseq --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "agvNHVfc4N5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 17:41:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from model import HuggingFaceQwen2ForCausalLM\n",
    "\n",
    "args = OmegaConf.create()\n",
    "args.llm_config = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "model = HuggingFaceQwen2ForCausalLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is attention mask in language model?k, it is not clear.  It may be an example of the \"attention mask\".\n",
      "One possible alternative is that attention mask in this example would be added to the model, not the model. In this case, the model is fed a mask that it cannot see, and it must generate the desired answer. The model is still trained on this mask.\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "*Is that correct? \n",
      "\n",
      "*Where did the attention mask in the paper go? What if the problem of attention mask is reduced to the mask being fed to the model instead?\n",
      "\n",
      "\n",
      "Yes, it is correct that attention masks are added to the model. In the paper, this is not explicitly described, but the authors argue that the mask is useful as a way to reduce the difficulty of\n",
      "_________________________________________________\n",
      "Why language models are successful? is a very interesting question, given that a lot of the work we do with language models is about improving the quality of representations for new data and we are doing it in the context of a completely new and different problem.\n",
      "\n",
      "That is, we are dealing with a very different set of models (e.g., the word2vec model, a simple model which is very good for word classification but not word representation).\n",
      "\n",
      "For these systems (for the simplest case of word2vec) I have been playing with various versions of word2vec, including the word2vec-2 model, the version which only trains the embeddings and ignores the hidden states of the embeddings.\n",
      "\n",
      "I am not sure how robust that is as we are in the context of representing new data\n",
      "_________________________________________________\n",
      "Which paper is fundamental in nowadays NLP? I am currently studying about NLP. And I came across this question to find the paper that is fundamental of NLP. Can someone give me a good paper for this purpose? I am sorry if the question is not asked in the right forum.\n",
      "You can see some of the best NLP papers are here: \n",
      "http://www.neurips.cc/papersPDF/11447.pdf\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"What is attention mask in language model?\",\n",
    "    \"Why language models are successful?\",\n",
    "    \"Which paper is fundamental in nowadays NLP?\",\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.llm_config)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tok_outs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "input_ids = tok_outs.input_ids\n",
    "attention_mask = tok_outs.attention_mask\n",
    "\n",
    "generate_ids = model.generate(input_ids, attention_mask=attention_mask,\n",
    "                              max_new_tokens=150, do_sample=True, top_k=50, top_p=0.95)\n",
    "gen_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "for _ in gen_texts:\n",
    "    print(_)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.4256,  4.8533,  3.4071,  ..., -4.8739, -4.8724, -4.8735],\n",
       "         [ 1.0334,  2.2287,  1.7907,  ..., -5.4180, -5.4191, -5.4189],\n",
       "         [ 7.9500,  5.3180,  4.9196,  ..., -4.9389, -4.9381, -4.9378],\n",
       "         ...,\n",
       "         [ 6.6885,  4.1731,  2.9295,  ..., -5.6208, -5.6225, -5.6211],\n",
       "         [ 4.6720,  2.8864,  2.7626,  ..., -5.7757, -5.7758, -5.7749],\n",
       "         [ 6.9488,  5.8876,  6.5815,  ..., -3.2889, -3.2888, -3.2876]],\n",
       "\n",
       "        [[ 7.8377,  3.2649,  2.3366,  ..., -5.3130, -5.3133, -5.3130],\n",
       "         [ 5.4114,  5.7092,  3.6707,  ..., -5.2595, -5.2586, -5.2577],\n",
       "         [ 7.5345,  7.0438,  3.4638,  ..., -4.9855, -4.9854, -4.9849],\n",
       "         ...,\n",
       "         [ 7.0046,  5.6227,  7.7066,  ..., -3.6718, -3.6726, -3.6713],\n",
       "         [ 7.2191,  5.7511,  7.5483,  ..., -3.6273, -3.6283, -3.6269],\n",
       "         [ 7.0624,  5.7042,  6.8024,  ..., -3.6658, -3.6667, -3.6655]],\n",
       "\n",
       "        [[ 4.6012,  2.8672,  1.2418,  ..., -4.7340, -4.7324, -4.7334],\n",
       "         [ 5.2055,  5.2384,  5.1805,  ..., -3.8021, -3.8019, -3.8027],\n",
       "         [ 2.2612,  4.2517,  2.6010,  ..., -5.0426, -5.0431, -5.0422],\n",
       "         ...,\n",
       "         [ 7.8117,  4.4671,  5.6315,  ..., -4.7784, -4.7773, -4.7774],\n",
       "         [ 8.1697,  6.0092,  6.1033,  ..., -5.2642, -5.2646, -5.2643],\n",
       "         [ 5.0500,  1.9037,  2.1297,  ..., -6.1501, -6.1503, -6.1495]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
