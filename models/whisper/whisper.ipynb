{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# !pip install torchinfo fairseq transformers huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Audio\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from model import HuggingFaceWhisperModel\n",
    "\n",
    "args = OmegaConf.create()\n",
    "args.asr_config = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = HuggingFaceWhisperModel.build_model(args, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем тестовый датасет\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "sampling_rate = sample['sampling_rate']\n",
    "\n",
    "waveform = torch.tensor(sample['array']).unsqueeze(0)  # Add batch dimension\n",
    "waveform = waveform.float()\n",
    "\n",
    "inputs = model.processor(waveform.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "waveform = inputs['input_features']\n",
    "\n",
    "sf.write('audio.wav',sample['array'], sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n",
       "           264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n",
       "          5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n",
       "          2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n",
       "            13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n",
       "           264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n",
       "           949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n",
       "          3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n",
       "         12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n",
       "           534, 10281,   934,   439,    11]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгенерировать токены\n",
    "\n",
    "model.generate(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгенерировать текст\n",
    "\n",
    "model.generate(waveform, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n",
       "            264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n",
       "           5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n",
       "           2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n",
       "             13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n",
       "            264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n",
       "            949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n",
       "           3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n",
       "          12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n",
       "            534, 10281,   934,   439,    11]]),\n",
       " None,\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# форвард пасс\n",
    "\n",
    "in_features = model.processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "prompt_ids = torch.tensor(model.processor.tokenizer.prefix_tokens).unsqueeze(0)\n",
    "model(src_tokens=in_features, tgt_tokens=prompt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all,\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгенерировать из текст аудиофайла\n",
    "\n",
    "model.generate(file='audio.wav', text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50259, 50360, 50364,  2221,    13,  2326,   388,   391,   307,\n",
       "           264, 50244,   295,   264,  2808,  5359,    11,   293,   321,   366,\n",
       "          5404,   281,  2928,   702, 14943,    13,  6966,   307,  2221,    13,\n",
       "          2326,   388,   391,   311,  9060,  1570,  1880,   813,   702,  1871,\n",
       "            13,   634,  5112,   505,   300,   412,   341, 42729,  3196,   295,\n",
       "           264,  1064,    11,   365,  5272,   293, 12904,  9256,   450, 10539,\n",
       "           949,   505,    11,  1034,  4680, 10117,   490,  3936,   293,  1080,\n",
       "          3542,  5160,   881, 26336,   281,   264,  1575,    13,   634,   575,\n",
       "         12525, 22618,  1968,  6144, 35617, 20084,  1756,   311,   589,   307,\n",
       "           534, 10281,   934,   439,    11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгенерировать токены из аудиофайла\n",
    "\n",
    "model.generate(file='audio.wav')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
